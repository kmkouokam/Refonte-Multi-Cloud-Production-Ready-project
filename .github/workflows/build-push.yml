name: Bootstrap + Build + Deploy (AWS & GCP)

on:
  push:
    branches:
      - flask-app
  workflow_dispatch:

permissions:
  contents: write
  id-token: write
  actions: write

env:
  IMAGE_NAME: flask-app

  # AWS
  AWS_CLUSTER_NAME: multi-cloud-cluster
  AWS_REGION: ${{ secrets.AWS_REGION }}
  AWS_ACCOUNT_ID: ${{ secrets.AWS_ACCOUNT_ID }}
  

  # GCP
  GCP_CLUSTER_NAME:  my-gcp-cluster-7393678df1c1
  GCP_REGION: us-east4
  GCP_PROJECT_ID: ${{ secrets.GCP_PROJECT_ID }}
  GCP_CREDENTIALS_JSON: ${{ secrets.GCP_CREDENTIALS_JSON }}
  PUBLIC_IP: ${{secrets.PUBLIC_IP}}
  GCP_REPO: my-repo
  GCP_PROJECT_NUMBER: ${{ secrets.GCP_PROJECT_NUMBER }}




jobs:
  aws-bootstrap:
    name: Bootstrap AWS (ArgoCD + Cluster)
    runs-on: [self-hosted, linux, vpc-runner]
     

    steps:
      - uses: actions/checkout@v4

      - name: Set job-scoped configs (AWS)
        shell: bash
        run: |
         set -euo pipefail

         echo "RUNNER_TEMP=$RUNNER_TEMP"
         mkdir -p "$RUNNER_TEMP/aws-config"

         echo "KUBECONFIG=$RUNNER_TEMP/kubeconfig-aws" >> "$GITHUB_ENV"
        # echo "AWS_SHARED_CREDENTIALS_FILE=$RUNNER_TEMP/aws-config" >> "$GITHUB_ENV"


      - name: Configure EKS kubeconfig
        run: |
          aws eks update-kubeconfig \
            --name $AWS_CLUSTER_NAME \
            --region $AWS_REGION

      - name: Verify AWS kubeconfig
        run: kubectl config get-contexts      

      - name: Install ArgoCD on AWS
        run: |
           kubectl create namespace argocd || true
           kubectl apply -n argocd \
             -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml --validate=false    

            kubectl apply \
             -f https://raw.githubusercontent.com/argoproj/argo-rollouts/stable/manifests/install.yaml
           kubectl wait --for=condition=ready pod -n argocd --selector=app.kubernetes.io/name=argocd-server --timeout=300s

            
          
           # Wait additionally until the argocd-cm exists
           until kubectl get configmap argocd-cm -n argocd >/dev/null 2>&1; do
             echo "Waiting for argocd-cm to be created..."
             sleep 5
           done

      - name: Wait for ArgoCD
        run: |
          kubectl wait deployment argocd-server \
            -n argocd \
            --for=condition=available \
            --timeout=300s

      - name: Install RBAC
        run: |
          git fetch origin gitop
          git checkout gitop
           cd argocd/k8s
           kubectl -n argocd apply -f argocd-rbac.yaml          
      

      - name: Login ArgoCD
        run: |
          kubectl -n argocd port-forward svc/argocd-server 8080:443 >/tmp/argocd-pf.log 2>&1 &
          PF_PID=$!
          sleep 10
          kill $PF_PID || true
          ARGO_PWD=$(kubectl -n argocd get secret argocd-initial-admin-secret \
            -o jsonpath="{.data.password}" | base64 -d)
            echo "ARGOCD_ADMIN_PASSWORD=$ARGO_PWD" >> $GITHUB_ENV

          argocd login localhost:8080 \
            --username admin \
            --password "$ARGO_PWD" \
            --insecure

      - name: Register AWS cluster
        run: |
          CONTEXT=$(kubectl config current-context)
          argocd cluster add "$CONTEXT" --label cloud=aws --upsert 

      - name: Clean up runner (safe)
        if: always()
        run: |
         echo "Cleaning up runner safely..."

         # Stop any background port-forwards to avoid port conflicts
         pkill -f "kubectl.*port-forward" || true

         # Remove only job-scoped configs (if defined)
         if [ -n "${KUBECONFIG:-}" ]; then
          rm -f "$KUBECONFIG" || true
         fi

         if [ -n "${CLOUDSDK_CONFIG:-}" ]; then
         rm -rf "$CLOUDSDK_CONFIG" || true
         fi

         # Clean job temp files only (never HOME)
         if [ -n "${env.RUNNER_TEMP:-}" ]; then
         rm -rf "${env.RUNNER_TEMP:?}/"* || true
         fi

         # Optional: Docker cleanup (keeps runner stable, slower builds)
         docker system prune -af || true
      
  gcp-bootstrap:
      name: Bootstrap GCP (ArgoCD + Cluster)
      needs: aws-bootstrap
      runs-on: [self-hosted, linux, vpc-runner]
       

      steps:
      - uses: actions/checkout@v4

      - name: Set job scoped KUBECONFIG
        shell: bash
        run: |
          set -euo pipefail
          echo "RUNNER_TEMP=$RUNNER_TEMP"
          mkdir -p "$RUNNER_TEMP/gcloud-config"

          echo "KUBECONFIG=${{ env.RUNNER_TEMP }}/kubeconfig-gcp" >> $GITHUB_ENV
          echo "CLOUDSDK_CONFIG=${{ env.RUNNER_TEMP }}/gcloud-config" >> $GITHUB_ENV
           

      # âœ… Authenticate FIRST
      - name: Authenticate to GCP
        uses: google-github-actions/auth@v2
        with:
         credentials_json: ${{ secrets.GCP_SA_KEY }}

      - name: Fix gcloud install location
        run: |
          echo "CLOUDSDK_ROOT_DIR=$HOME/google-cloud-sdk" >> $GITHUB_ENV
           echo "PATH=$HOME/google-cloud-sdk/bin:$PATH" >> $GITHUB_ENV

      - name: Setup gcloud
        uses: google-github-actions/setup-gcloud@v3
        with:
          project_id: ${{ env.GCP_PROJECT_ID }}
          install_components: kubectl,gke-gcloud-auth-plugin

      - name: Verify gcloud
        run: |
         gcloud auth list
         gcloud version
         kubectl version --client    

      # --- Install gcloud ---
      # - name: Install Google Cloud SDK
      #   run: |
      #     # Create safe directory
      #     mkdir -p /home/ec2-user/gcloud
      #     cd /home/ec2-user/gcloud
      #     # Detect package manager
      #     if command -v apt-get >/dev/null 2>&1; then
      #      sudo apt-get update -y
      #      sudo apt-get install -y apt-transport-https ca-certificates gnupg curl
      #     elif command -v yum >/dev/null 2>&1 || command -v dnf >/dev/null 2>&1; then
      #      sudo yum update -y || sudo dnf update -y
      #      sudo yum install -y curl ca-certificates || sudo dnf install -y curl ca-certificates
      #     else
      #       echo "No supported package manager found"
      #      exit 1
      #     fi

      #      # If using apt-get, install Google Cloud SDK via apt repo
      #     if command -v apt-get >/dev/null 2>&1; then
      #       curl -fsSL https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo gpg --dearmor -o /usr/share/keyrings/cloud.google.gpg
      #       echo "deb [signed-by=/usr/share/keyrings/cloud.google.gpg] https://packages.cloud.google.com/apt cloud-sdk main" | \
      #       sudo tee /etc/apt/sources.list.d/google-cloud-sdk.list
      #       sudo apt-get update
      #       sudo apt-get install -y google-cloud-sdk

      #       # Install GKE auth plugin
      #      sudo apt-get install -y google-cloud-cli-gke-gcloud-auth-plugin
      #     else
      #     # For Amazon Linux 2023 or other yum/dnf distros, use Google's install script
      #       sudo -u ec2-user bash
      #       cd ~
      #       curl -O https://dl.google.com/dl/cloudsdk/channels/rapid/downloads/google-cloud-cli-linux-x86_64.tar.gz  
      #       tar -xf google-cloud-cli-linux-x86_64.tar.gz
      #        ./google-cloud-sdk/install.sh --quiet
      #       echo 'source $HOME/google-cloud-sdk/path.bash.inc' >> ~/.bashrc
      #       echo 'source $HOME/google-cloud-sdk/completion.bash.inc' >> ~/.bashrc
      #       source ~/.bashrc

            # Install GKE auth plugin
      - name: Ensure GKE auth plugin is installed
        run: |
           gcloud components install gke-gcloud-auth-plugin --quiet
           fi
    

       

      - name: Update kubeconfig GCP
        run: |
          gcloud container clusters update "$GCP_CLUSTER_NAME" \
            --region "$GCP_REGION" \
            --project "$GCP_PROJECT_ID" \
            --enable-master-authorized-networks \
            --master-authorized-networks 0.0.0.0/0     

      - name: Update GKE kubeconfig
        run: |
          echo "Cluster: $GCP_CLUSTER_NAME, Region: $GCP_REGION, Project: $GCP_PROJECT_ID"
          gcloud container clusters get-credentials "$GCP_CLUSTER_NAME" \
            --region "$GCP_REGION" \
            --project "$GCP_PROJECT_ID" \
            

      - name: Verify GCP kubeconfig
        run: kubectl config get-contexts   
        
      # --- Optional: Check connectivity ---
      - name: Check Kubernetes API connectivity
        run: |
         INTERNAL_IP=$(kubectl config view --minify -o jsonpath='{.clusters[0].cluster.server}' | sed 's|https://||')
         if ! curl -k -m 5 https://$INTERNAL_IP; then
          echo "Cannot reach Kubernetes API via internal IP. Check VPC/firewall!"
          exit 1
         fi  

      

      - name: Install ArgoCD on GCP
        run: |
           kubectl create namespace argocd || true
           kubectl apply -n argocd \
             -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml --validate=false    

            kubectl apply \
             -f https://raw.githubusercontent.com/argoproj/argo-rollouts/stable/manifests/install.yaml
           kubectl wait --for=condition=ready pod -n argocd --selector=app.kubernetes.io/name=argocd-server --timeout=300s

            
          
           # Wait additionally until the argocd-cm exists
           until kubectl get configmap argocd-cm -n argocd >/dev/null 2>&1; do
             echo "Waiting for argocd-cm to be created..."
             sleep 5
           done

      - name: Wait for ArgoCD
        run: |
          kubectl wait deployment argocd-server \
            -n argocd \
            --for=condition=available \
            --timeout=300s
            
         
      - name: Install RBAC
        run: |
          git fetch origin gitop
          git checkout gitop
           cd argocd/k8s
           kubectl -n argocd apply -f argocd-rbac.yaml          

      - name: Register GCP cluster
        run: |
           kubectl -n argocd wait pod -l app.kubernetes.io/name=argocd-server --for=condition=Ready --timeout=120s
           kubectl -n argocd port-forward svc/argocd-server 8080:443 &
           sleep 10
           ARGO_PWD=$(kubectl -n argocd get secret argocd-initial-admin-secret \
            -o jsonpath="{.data.password}" | base64 -d)
            echo "ARGOCD_ADMIN_PASSWORD=$ARGO_PWD" >> $GITHUB_ENV

           argocd login localhost:8080 \
            --username admin \
            --password "$ARGO_PWD" \
            --insecure

            #Register GCP cluster
           CONTEXT=$(kubectl config current-context)
           argocd cluster add "$CONTEXT" --label cloud=gcp --upsert

      - name: Clean up runner (safe)
        if: always()
        run: |
         echo "Cleaning up runner safely..."

         # Stop any background port-forwards to avoid port conflicts
         pkill -f "kubectl.*port-forward" || true

         # Remove only job-scoped configs (if defined)
         if [ -n "${KUBECONFIG:-}" ]; then
          rm -f "$KUBECONFIG" || true
         fi

         if [ -n "${CLOUDSDK_CONFIG:-}" ]; then
         rm -rf "$CLOUDSDK_CONFIG" || true
         fi

         # Clean job temp files only (never HOME)
         if [ -n "${env.RUNNER_TEMP:-}" ]; then
         rm -rf "${env.RUNNER_TEMP:?}/"* || true
         fi

         # Optional: Docker cleanup (keeps runner stable, slower builds)
         docker system prune -af || true
     




  aws-build-push:
      name: Build & Deploy AWS
      needs: gcp-bootstrap
      runs-on: [self-hosted, linux, vpc-runner]




      steps:
      - uses: actions/checkout@v4

      - name: Set job scoped KUBECONFIG
        run: |
          set -euo pipefail
          echo "RUNNER_TEMP=$RUNNER_TEMP"
          mkdir -p "$RUNNER_TEMP/aws-config"

          echo "KUBECONFIG=${{ env.RUNNER_TEMP }}/kubeconfig-aws" >> $GITHUB_ENV
           

      - name: Check AWS Identity (Self-Hosted Runner)
        run: aws sts get-caller-identity

      - name: Login to ECR
        run: |
          aws ecr get-login-password --region $AWS_REGION |
          docker login --username AWS --password-stdin \
            $AWS_ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com

      - name: Build & Push Image (AWS)
        run: |
          docker build -t $IMAGE_NAME -f flask_app/Dockerfile .
          docker tag $IMAGE_NAME:latest \
            $AWS_ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com/$IMAGE_NAME:latest
            
          docker push \
            $AWS_ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com/$IMAGE_NAME:latest


      - name: Set IMAGE_URI for AWS
        run: |
          echo "IMAGE_URI=${AWS_ACCOUNT_ID}.dkr.ecr.${AWS_REGION}.amazonaws.com/${IMAGE_NAME}:latest" >> $GITHUB_ENV 
          
      
            
      - name: Create ECR Pull Secret
        run: |
          kubectl create secret docker-registry ecr-creds \
          --docker-server=$AWS_ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com \
          --docker-username=AWS \
          --docker-password=$(aws ecr get-login-password --region $AWS_REGION) \
          -n default || true 

      - name: Update kubeconfig AWS
        run: |
          aws eks update-kubeconfig --name "$AWS_CLUSTER_NAME" --region "$AWS_REGION"
          kubectl config current-context
          kubectl get nodes -o wide

          
      - name: Install Helm
        run: |
          curl -fsSL https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 | bash 
          
      #   Detect VPC ID
      - name: Fetch VPC ID
        run: |
          echo "Fetching VPC ID for cluster..."
          VPC_ID=$(aws eks describe-cluster --name $AWS_CLUSTER_NAME --region $AWS_REGION \
          --query "cluster.resourcesVpcConfig.vpcId" --output text)
          echo "VPC_ID=$VPC_ID" >> $GITHUB_ENV

      - name: Cleanup stuck AWS Load Balancer Controller (safe)
        run: |
         set +e

          echo "Checking Helm release status..."
          helm status aws-load-balancer-controller -n kube-system

          echo "Attempting safe uninstall (if exists)..."
           helm uninstall aws-load-balancer-controller -n kube-system || true

          echo "Removing any stuck Helm secrets..."
          kubectl -n kube-system delete secret -l "name=aws-load-balancer-controller,owner=helm" || true

         set -e

       # Add Helm repo
      - name: Add Helm repo for AWS charts
        run: |
          helm repo add eks https://aws.github.io/eks-charts
          helm repo update 
          kubectl create namespace kube-system --dry-run=client -o yaml | kubectl apply -f - 
    
      - name: Associate IAM OIDC Provider
        run: |
          eksctl utils associate-iam-oidc-provider \
          --region $AWS_REGION \
          --cluster $AWS_CLUSTER_NAME \
          --approve    
       
      - name: AWS Cloudformation Setup for ALB Controller
        run: |
          aws cloudformation describe-stack-events \
          --stack-name eksctl-multi-cloud-cluster-addon-iamserviceaccount-kube-system-aws-load-balancer-controller \
          --max-items 20



      - name: Create service account for AWS Load Balancer Controller
        run: |
          eksctl create iamserviceaccount \
          --cluster $AWS_CLUSTER_NAME \
          --namespace kube-system \
          --name aws-load-balancer-controller \
          --attach-policy-arn arn:aws:iam::435329769674:policy/AWSLoadBalancerControllerIAMPolicy \
          --override-existing-serviceaccounts \
          --region $AWS_REGION \
          --approve

      - name: Verify ServiceAccount exists (EKS)
        run: |
         kubectl -n kube-system get sa aws-load-balancer-controller -o yaml    

      # Apply CRDs explicitly (recommended for upgrade/install safety)
      - name: Install AWS Load Balancer Controller CRDs
        run: |
         wget -q https://raw.githubusercontent.com/aws/eks-charts/master/stable/aws-load-balancer-controller/crds/crds.yaml
         kubectl apply -f crds.yaml    

      
          
#

      - name: Install AWS Load Balancer Controller
        run: |
         helm upgrade --install aws-load-balancer-controller eks/aws-load-balancer-controller \
         -n kube-system \
          --set clusterName="$AWS_CLUSTER_NAME" \
         --set serviceAccount.create=false \
         --set serviceAccount.name=aws-load-balancer-controller \
        #  --set region="$AWS_REGION" \
        #  --set vpcId="$VPC_ID" \
        #  --wait \
        #  --set crds.create=false
        #   --timeout 20m \
        #    --debug




        
          
       

      #  Wait for controller to be ready
      - name: Wait for AWS Load Balancer Controller to be healthy
        run: |
          kubectl wait --for=condition=available deployment/aws-load-balancer-controller \
          -n kube-system --timeout=300s

      # Fetch db credentials from terraform outputs
      - name: Fetch DB credentials from terraform outputs
        run: |
          # chackout master to fetch latest terraform outputs
          git fetch origin master
          git checkout master 
          # AWS DB credentials
          export AWS_DB_USERNAME=$(terraform output -raw aws_db_username)
          export AWS_DB_PASSWORD=$(terraform output -raw aws_db_password)
          export AWS_DB_HOST=$(terraform output -raw aws_db_host)
          export AWS_DB_NAME=$(terraform output -raw aws_db_name)

          # Persist to GitHub Actions environment
          echo "AWS_DB_USERNAME=$AWS_DB_USERNAME" >> $GITHUB_ENV
          echo "AWS_DB_PASSWORD=$AWS_DB_PASSWORD" >> $GITHUB_ENV
          echo "AWS_DB_HOST=$AWS_DB_HOST" >> $GITHUB_ENV
          echo "AWS_DB_NAME=$AWS_DB_NAME" >> $GITHUB_ENV 
          
          # Apply Kubernetes Secret and RBAC
      - name: Apply Kubernetes Secret
        run: |
          git fetch origin gitop
          git checkout gitop
           cd argocd/k8s
           envsubst < secret-aws.yaml | kubectl apply -f -
           envsubst < argocd-rbac.yaml | kubectl apply -f -

      - name: Apply ArgoCD ApplicationSet (AWS)
        run: |
          git fetch origin gitop
          git checkout gitop  
          cd argocd/aws
          kubectl apply -f flask-app-rollout.yaml --namespace=default

      - name: deploy ingress-aws and service-aws
        run: |
          git fetch origin gitop
          git checkout gitop
          cd argocd/k8s
          kubectl apply -f ingress-aws.yaml 
          kubectl apply -f service-aws.yaml

      - name: Login ArgoCD
        run: |
          kubectl -n argocd port-forward svc/argocd-server 8080:443 >/tmp/argocd-pf.log 2>&1 &
          PF_PID=$!
          sleep 10
          kill $PF_PID || true


          ARGO_PWD=$(kubectl -n argocd get secret argocd-initial-admin-secret \
            -o jsonpath="{.data.password}" | base64 -d)
            echo "ARGOCD_ADMIN_PASSWORD=$ARGO_PWD" >> $GITHUB_ENV

          argocd login localhost:8080 \
            --username admin \
            --password "$ARGO_PWD" \
            --insecure    

      - name: Create or upsert argocd cluster AWS
        run: |
           argocd app create flask-app \
           --port-forward \
           --repo https://github.com/${{ github.repository }} \
           --path argocd/aws \
           --dest-server https://kubernetes.default.svc \
           --dest-namespace default \
           --project default \
           --sync-policy automated \
           --auto-prune \
           --self-heal \
           --upsert \
           --revision gitop

      - name: Update ArgoCD image
        run: |
             argocd app set flask-app \
               --param IMAGE_URI=$IMAGE_URI

               argocd app sync flask-app

      - name: Clean up runner (safe)
        if: always()
        run: |
         echo "Cleaning up runner safely..."

         # Stop any background port-forwards to avoid port conflicts
         pkill -f "kubectl.*port-forward" || true

         # Remove only job-scoped configs (if defined)
         if [ -n "${KUBECONFIG:-}" ]; then
          rm -f "$KUBECONFIG" || true
         fi

         if [ -n "${CLOUDSDK_CONFIG:-}" ]; then
         rm -rf "$CLOUDSDK_CONFIG" || true
         fi

         # Clean job temp files only (never HOME)
         if [ -n "${env.RUNNER_TEMP:-}" ]; then
         rm -rf "${env.RUNNER_TEMP:?}/"* || true
         fi

         # Optional: Docker cleanup (keeps runner stable, slower builds)
         docker system prune -af || true
          



  gcp-build-push:
    name: Build & Deploy GCP
    needs: [gcp-bootstrap, aws-build-push]
    environment: production
    runs-on: [self-hosted, linux, vpc-runner]
     
 
    steps:
      - uses: actions/checkout@v4

      - name: Set job scoped KUBECONFIG
        shell: bash
        run: |
          set -euo pipefail
          echo "RUNNER_TEMP=$RUNNER_TEMP"
          mkdir -p "$RUNNER_TEMP/gcloud-config"

          echo "KUBECONFIG=${{ env.RUNNER_TEMP }}/kubeconfig-gcp" >> $GITHUB_ENV
          echo "CLOUDSDK_CONFIG=${{ env.RUNNER_TEMP }}/gcloud-config" >> $GITHUB_ENV
           

      - name: Authenticate to GCP
        uses: google-github-actions/auth@v3
        with:
          credentials_json: ${{ secrets.GCP_CREDENTIALS_JSON }}
          project_id: ${{ env.GCP_PROJECT_ID }}

      - name: Authenticate to GCP and Install GKE auth plugin
        run: |
          # Install gke plugin
           gcloud components install gke-gcloud-auth-plugin --quiet 
          
          echo "${{ secrets.GCP_SA_KEY }}" | base64 --decode > key.json
          gcloud auth activate-service-account --key-file=key.json  --quiet
          # Verify
            gcloud version
            kubectl version --client    

       #  Clean and reconfigure Docker auth for Artifact Registry
      - name: Clean old Docker logins (optional safeguard)
        run: |
           docker logout https://${{ env.GCP_REGION }}-docker.pkg.dev || true 
           
      - name: Ensure Artifact Registry repo exists
        run: |
          if ! gcloud artifacts repositories describe my-repo --location=${{ env.GCP_REGION }} >/dev/null 2>&1; then
           gcloud artifacts repositories create my-repo \
           --repository-format=docker \
          --location=${{ env.GCP_REGION }} \
           --description="Docker repo for my project"
          echo "Waiting for repo to be ready..."
           sleep 15
           fi
           echo "Repository my-repo already exists."


      - name: Configure Docker for Artifact Registry and login
        run: |
          docker logout https://${{ env.GCP_REGION }}-docker.pkg.dev || true
          gcloud auth configure-docker $GCP_REGION-docker.pkg.dev --quiet

      - name: Fallback Docker login
        if: failure()
        run: |
          gcloud auth print-access-token | \
          docker login -u oauth2accesstoken --password-stdin https://${{ env.GCP_REGION }}-docker.pkg.dev 




      - name: Build & Push Image (GCP)
        run: |
          docker build -t $IMAGE_NAME -f flask_app/Dockerfile .
          docker tag $IMAGE_NAME:latest \
            $GCP_REGION-docker.pkg.dev/$GCP_PROJECT_ID/my-repo/$IMAGE_NAME:latest

            docker push \
            $GCP_REGION-docker.pkg.dev/$GCP_PROJECT_ID/my-repo/$IMAGE_NAME:latest

      - name: Set IMAGE_URI for GCP
        run: |
          echo "IMAGE_URI=${GCP_REGION}-docker.pkg.dev/${GCP_PROJECT_ID}/my-repo/${IMAGE_NAME}:latest" >> $GITHUB_ENV

      - name: Update kubeconfig GCP
        run: |
          gcloud container clusters update "$GCP_CLUSTER_NAME" \
            --region "$GCP_REGION" \
            --project "$GCP_PROJECT_ID" \
            --enable-master-authorized-networks \
            --master-authorized-networks 0.0.0.0/0   

      - name: Update kubeconfig GCP
        run: |
          gcloud container clusters get-credentials "$GCP_CLUSTER_NAME" \
            --region "$GCP_REGION" \
            --project "$GCP_PROJECT_ID"
             

      - name: Create GCR Pull Secret
        run: |
          # Full registry URL
            REGISTRY_URL="${GCP_REGION}-docker.pkg.dev/${GCP_PROJECT}/${GCP_REPO}"
           kubectl create secret docker-registry gcp-creds \
           --docker-server="${REGISTRY_URL}" \
           --namespace default \
           --docker-username=_json_key \
           --docker-password="$(echo "${{ secrets.GCP_SA_KEY }}" | base64 --decode)" \
           --dry-run=client -o yaml | kubectl apply -f -     
           
      - name: Tecth DB credentials from terraform outputs
        run: |
          # chackout master to fetch latest terraform outputs
          git fetch origin master
          git checkout master 
          # GCP DB credentials
          export GCP_DB_USERNAME=$(terraform output -raw gcp_db_username)
          export GCP_DB_PASSWORD=$(terraform output -raw gcp_db_password)
          export GCP_DB_HOST=$(terraform output -raw gcp_db_host)
          export GCP_DB_NAME=$(terraform output -raw gcp_db_name)

          # Persist to GitHub Actions environment
          echo "GCP_DB_USERNAME=$GCP_DB_USERNAME" >> $GITHUB_ENV
          echo "GCP_DB_PASSWORD=$GCP_DB_PASSWORD" >> $GITHUB_ENV
          echo "GCP_DB_HOST=$GCP_DB_HOST" >> $GITHUB_ENV
          echo "GCP_DB_NAME=$GCP_DB_NAME" >> $GITHUB_ENV 
          
          # Apply Kubernetes Secret and RBAC
      - name: Apply Kubernetes Secret
        run: |
          git fetch origin gitop
          git checkout gitop
           cd argocd/k8s
           envsubst < secret-gcp.yaml | kubectl apply -f -
           envsubst < argocd-rbac.yaml | kubectl apply -f -
            
      - name: Apply ArgoCD ApplicationSet (GCP)
        run: |
          git fetch origin gitop
          git checkout gitop  
          cd argocd/gcp
          kubectl apply -f flask-app-rollout.yaml -n argocd   
          
      - name: deploy ingress-gcp and service-gcp
        run: |
          git fetch origin gitop
          git checkout gitop
          cd argocd/k8s
          kubectl apply -f ingress-gcp.yaml 
          kubectl apply -f service-gcp.yaml  
          
          
      - name: Clean up runner (safe)
        if: always()
        run: |
         echo "Cleaning up runner safely..."

         # Stop any background port-forwards to avoid port conflicts
         pkill -f "kubectl.*port-forward" || true

         # Remove only job-scoped configs (if defined)
         if [ -n "${KUBECONFIG:-}" ]; then
          rm -f "$KUBECONFIG" || true
         fi

         if [ -n "${CLOUDSDK_CONFIG:-}" ]; then
         rm -rf "$CLOUDSDK_CONFIG" || true
         fi

         # Clean job temp files only (never HOME)
         if [ -n "${env.RUNNER_TEMP:-}" ]; then
         rm -rf "${env.RUNNER_TEMP:?}/"* || true
         fi

         # Optional: Docker cleanup (keeps runner stable, slower builds)
         docker system prune -af || true

































# name: Build and Push Flask App Image #

# on:
#   repository_dispatch:
#     types:
#       - run_build
     
# permissions:
#   contents: write
#   id-token: write
#   actions: write

# env:
#   IMAGE_NAME: flask-app

# jobs:
#   # ----------------------------------------
#   # 1ï¸âƒ£ AWS JOB
#   # ----------------------------------------
#   aws-build-push:
#     if: ${{ github.event.client_payload.branch == 'flask-app' }}
#     runs-on: [self-hosted, linux, vpc-runner]
#     environment: production

#     env:
#       AWS_REGION: ${{ secrets.AWS_REGION }}
#       AWS_ACCOUNT_ID: ${{ secrets.AWS_ACCOUNT_ID }}
#       ARGOCD_ADMIN_PASSWORD: ${{secrets.ARGOCD_ADMIN_PASSWORD}}
#       # ARGOCD_SERVER: localhost:8080

#     steps:
#       - name: Checkout code
#         uses: actions/checkout@v4

#       - name: Check AWS Identity (Self-Hosted Runner)
#         run: aws sts get-caller-identity

#       - name: Debug dispatch payload
#         run: |
#          echo "Triggered by dispatch"
#          echo "Branch: ${{ github.event.client_payload.branch }}"
#          echo "SHA: ${{ github.event.client_payload.sha }}"
  

#       - name: Login to AWS ECR
#         run: |
#           aws ecr get-login-password --region $AWS_REGION |
#           docker login --username AWS --password-stdin \
#           $AWS_ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com

#       # Build Docker Image
#       - name: Build Image
#         run: |
#           docker build -t $IMAGE_NAME -f flask_app/Dockerfile .
#           docker tag $IMAGE_NAME:latest \
#             $AWS_ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com/$IMAGE_NAME:${GITHUB_SHA}

#       - name: Push Image to AWS ECR
#         run: |
#           docker push \
#             $AWS_ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com/$IMAGE_NAME:${GITHUB_SHA}

#       - name: Set IMAGE_URI for AWS
#         run: |
#           echo "IMAGE_URI=${AWS_ACCOUNT_ID}.dkr.ecr.${AWS_REGION}.amazonaws.com/${IMAGE_NAME}:${GITHUB_SHA}" >> $GITHUB_ENV

#       # Configure EKS
#       - name: Update kubeconfig for AWS EKS
#         env:
#           AWS_REGION: ${{ env.AWS_REGION }}
#           CLUSTER_NAME: multi-cloud-cluster
#         run: |
#           aws eks update-kubeconfig \
#             --name multi-cloud-cluster \
#             --region $AWS_REGION \
#              --alias eks-prod \

#       - name: Create ECR Pull Secret
#         run: |
#           kubectl create secret docker-registry ecr-creds \
#           --docker-server=$AWS_ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com \
#           --docker-username=AWS \
#           --docker-password=$(aws ecr get-login-password --region $AWS_REGION) \
#           -n default || true

#       - name: Install Helm
#         run: |
#           curl -fsSL https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 | bash

#       # 1ï¸âƒ£ Detect VPC ID
#       - name: Fetch VPC ID
#         run: |
#           echo "Fetching VPC ID for cluster..."
#           VPC_ID=$(aws eks describe-cluster --name multi-cloud-cluster --region $AWS_REGION \
#           --query "cluster.resourcesVpcConfig.vpcId" --output text)
#           echo "VPC_ID=$VPC_ID" >> $GITHUB_ENV

#       # 2ï¸âƒ£ Install CRDs
#       - name: Install AWS Load Balancer Controller CRDs
#         run: |
#           echo "Installing AWS Load Balancer Controller CRDs..."
#            kubectl apply -f https://raw.githubusercontent.com/aws/eks-charts/master/stable/aws-load-balancer-controller/crds/crds.yaml --validate=false

#       # 3ï¸âƒ£ Add Helm repo
#       - name: Add Helm repo for AWS charts
#         run: |
#           helm repo add eks https://aws.github.io/eks-charts
#           helm repo update

#       #  Install or upgrade AWS Load Balancer Controller
#       - name: Install AWS Load Balancer Controller via Helm
#         run: |
#           echo "Installing AWS Load Balancer Controller..."
#           helm upgrade --install aws-load-balancer-controller eks/aws-load-balancer-controller \
#            -n kube-system \
#            --set clusterName=multi-cloud-cluster \
#           --set serviceAccount.create=true \
#           --set region=$AWS_REGION \
#           --set vpcId=$VPC_ID \
#           --set crds.create=false

#       #  Wait for controller to be ready
#       - name: Wait for AWS Load Balancer Controller to be healthy
#         run: |
#           kubectl wait --for=condition=available deployment/aws-load-balancer-controller \
#           -n kube-system --timeout=300s

#       # - name: Port-forward ArgoCD server
#       #   run: |
#       #    kubectl -n argocd port-forward svc/argocd-server 8080:443 &
#       #     sleep 10
# #
#       # - name: Install ArgoCD on AWS
#       #   run: |
#       #     kubectl create namespace argocd || true
#       #     kubectl apply -n argocd \
#       #       -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml --validate=false

#       #       kubectl apply \
#       #       -f https://raw.githubusercontent.com/argoproj/argo-rollouts/stable/manifests/install.yaml
#       #     kubectl wait --for=condition=ready pod -n argocd --selector=app.kubernetes.io/name=argocd-server --timeout=300s

#       #     # Wait additionally until the argocd-cm exists
#       #     until kubectl get configmap argocd-cm -n argocd >/dev/null 2>&1; do
#       #       echo "Waiting for argocd-cm to be created..."
#       #       sleep 5
#       #     done

#       - name: Fetch DB credentials from terraform outputs
#         run: |
#           # chackout master to fetch latest terraform outputs
#           git fetch origin master
#           git checkout master 
#           # AWS DB credentials
#           export AWS_DB_USERNAME=$(terraform output -raw aws_db_username)
#           export AWS_DB_PASSWORD=$(terraform output -raw aws_db_password)
#           export AWS_DB_HOST=$(terraform output -raw aws_db_host)
#           export AWS_DB_NAME=$(terraform output -raw aws_db_name)

#           # Persist to GitHub Actions environment
#           echo "AWS_DB_USERNAME=$AWS_DB_USERNAME" >> $GITHUB_ENV
#           echo "AWS_DB_PASSWORD=$AWS_DB_PASSWORD" >> $GITHUB_ENV
#           echo "AWS_DB_HOST=$AWS_DB_HOST" >> $GITHUB_ENV
#           echo "AWS_DB_NAME=$AWS_DB_NAME" >> $GITHUB_ENV

#       - name: Apply Kubernetes Secret
#         run: |
#           git fetch origin gitop
#           git checkout gitop
#            cd argocd/k8s
#            envsubst < secret-aws.yaml | kubectl apply -f -
#            envsubst < argocd-rbac.yaml | kubectl apply -f -

#       - name: Apply ArgoCD ApplicationSet (AWS)
#         run: |
#           git fetch origin gitop
#           git checkout gitop  
#           cd argocd/aws
#           kubectl apply -f flask-app-rollout.yaml -n argocd --namespace=default

#       - name: deploy ingress-aws and service-aws
#         run: |
#           git fetch origin gitop
#           git checkout gitop
#           cd argocd/k8s
#           kubectl apply -f ingress-aws.yaml 
#           kubectl apply -f service-aws.yaml

#       - name: Update kubeconfig AWS
#         run: |
#           aws eks update-kubeconfig \
#             --name multi-cloud-cluster \
#             --region $AWS_REGION
#         env:
#           AWS_REGION: ${{ env.AWS_REGION }}

#       # - name: Apply Kubernetes Rollout (AWS)
#       #   run: |
#       #     git fetch origin gitop
#       #     git checkout gitop
#       #     cd argocd
#       #     envsubst < flask-app-aws-rollout.yaml | kubectl apply -f -

#       # - name: Fetch ArgoCD admin password
#       #   run: |
#       #    ARGO_PWD=$(kubectl -n argocd get secret argocd-initial-admin-secret \
#       #    -o jsonpath="{.data.password}" | base64 -d)
#       #     echo "ARGOCD_ADMIN_PASSWORD=$ARGO_PWD" >> $GITHUB_ENV

#       # - name: ArgoCD login
#       #   run: |
#       #     echo "Using ArgoCD server: localhost:8080"
#       #     argocd login localhost:8080 \
#       #     --username admin \
#       #     --password $ARGOCD_ADMIN_PASSWORD \
#       #     --insecure

#       # - name: Wait for ArgoCD core config
#       #   run: |
#       #    echo "Waiting for argocd-cm..."
#       #     until kubectl get configmap argocd-cm -n argocd >/dev/null 2>&1; do
#       #      sleep 5
#       #    done

#       #     echo "Waiting for argocd-secret..."
#       #    until kubectl get secret argocd-secret -n argocd >/dev/null 2>&1; do
#       #    sleep 5
#       #    done

#       #    echo "ArgoCD core components ready"

#       # - name: Create or Upsert ArgoCD App (AWS)
#       #   run: |
#       #     argocd app create flask-app-aws \
#       #      --repo https://github.com/kmkouokam/Refonte-Multi-Cloud-Production-Ready-project.git  \
#       #      --path argocd \
#       #      --revision gitop \
#       #      --dest-server https://kubernetes.default.svc \
#       #      --dest-namespace default \
#       #      --upsert \
#       #       --auto-prune \
#       #       --sync-policy automated \
#       #      --self-heal \
#       #      --directory-recurse

#       # - name: Update ArgoCD App Image (AWS)
#       #   run: |
#       #     echo "Updating ArgoCD deployment image to $IMAGE_URI ..."
#       #     argocd app set flask-app-aws \
#       #     --image $IMAGE_URI \
#       #       --timeout 60 \

#       #     argocd app sync flask-app-aws

#       # ----------------------------------------
#       #
#       # ðŸš¨ SLACK NOTIFICATION FOR AWS JOB
#       # ----------------------------------------
#       - name: Slack Notify AWS Job
#         if: always()
#         env:
#           SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}
#         run: |
#           STATUS="${{ job.status }}"
#           COMMIT=$GITHUB_SHA
#           REPO=$GITHUB_REPOSITORY
#           JOB="AWS Build & Deploy"

#           curl -X POST -H 'Content-type: application/json' \
#             --data "{
#               \"text\": \"*$JOB* finished with status: *$STATUS*\nRepo: $REPO\nCommit: $COMMIT\"
#             }" \
#             $SLACK_WEBHOOK_URL
#   #
#   # ----------------------------------------
#   # 2ï¸âƒ£ GCP JOB
#   # ----------------------------------------
#   gcp-build-push:
#     if: ${{ github.event.client_payload.branch == 'flask-app' }}
#     runs-on: [self-hosted, linux, vpc-runner]
#     # needs: aws-build-push   # wait for AWS job to finish before starting GCP job

#     env:
#       GCP_PROJECT_ID: ${{ secrets.GCP_PROJECT_ID }}
#       GCP_PROJECT_NUMBER: ${{ secrets.GCP_PROJECT_NUMBER }}
#       GCP_CREDENTIALS_JSON: ${{ secrets.GCP_CREDENTIALS_JSON }}
#       IMAGE_NAME: flask-app
#       GCP_REGION: us-east4
#       GCP_REPO: my-repo
#       GCP_CLUSTER_NAME: my-gcp-cluster-5e704c9d76a3
#       PUBLIC_IP: ${{secrets.PUBLIC_IP}}
#       # ARGOCD_SERVER: localhost:8080

#     steps:
#       - name: Debug dispatch payload
#         run: |
#          echo "Triggered by dispatch"
#           echo "Branch: ${{ github.event.client_payload.branch }}"
#           echo "SHA: ${{ github.event.client_payload.sha }}"


#       - name: Checkout code
#         uses: actions/checkout@v4

#       # Authenticate to GCP
#       - name: Authenticate to GCP
#         uses: google-github-actions/auth@v3
#         with:
#           credentials_json: ${{ env.GCP_CREDENTIALS_JSON }}
#           project_id: ${{ env.GCP_PROJECT_ID }}
#           # workload_identity_provider: ${{ secrets.WORKLOAD_IDENTITY_PROVIDER }}
#           # service_account: "refonte-project@prod-251618-359501.iam.gserviceaccount.com"

#       # - name: Fix gcloud install location
#       #   run: |
#       #     echo "CLOUDSDK_ROOT_DIR=$HOME/google-cloud-sdk" >> $GITHUB_ENV
#       #      echo "PATH=$HOME/google-cloud-sdk/bin:$PATH" >> $GITHUB_ENV

#       #   # 3ï¸âƒ£ Install gcloud CLI and kubectl
#       # - name: Setup gcloud CLI
#       #   uses: google-github-actions/setup-gcloud@v3
#       #   with:
#       #     project_id: ${{ env.GCP_PROJECT_ID }}
#       #     install_components: "kubectl"

#       # - name: Setup gcloud cloud SDK
#       #   uses: google-github-actions/setup-gcloud@v3
#       #   with:
#       #     project_id: ${{ env.GCP_PROJECT_ID }}
#       #     service_account_key: ${{ secrets.GCP_SA_KEY }}
#       #     export_default_credentials: true
#       # - name: Get GKE credentials
#       #   run: |
#       #     gcloud container clusters get-credentials ${{ env.GCP_CLUSTER_NAME }} \
#       #     --region ${{ env.GCP_REGION }} \
#       #     --project ${{ env.GCP_PROJECT_ID }}

#       #   # 4ï¸âƒ£ Optional: Check kubectl context
#       # - name: Check kubectl context
#       #   run: kubectl config current-context

#       # - name: Install Google Cloud SDK
#       #   run: |
#       #     # Create safe directory
#       #     mkdir -p /home/ec2-user/gcloud
#       #     cd /home/ec2-user/gcloud 
#       #     # Detect package manager
#       #     if command -v apt-get >/dev/null 2>&1; then
#       #      sudo apt-get update -y
#       #      sudo apt-get install -y apt-transport-https ca-certificates gnupg curl
#       #     elif command -v yum >/dev/null 2>&1 || command -v dnf >/dev/null 2>&1; then
#       #      sudo yum update -y || sudo dnf update -y
#       #      sudo yum install -y curl ca-certificates || sudo dnf install -y curl ca-certificates
#       #     else
#       #       echo "No supported package manager found"
#       #      exit 1
#       #     fi

#       #      # If using apt-get, install Google Cloud SDK via apt repo
#       #     if command -v apt-get >/dev/null 2>&1; then
#       #       curl -fsSL https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo gpg --dearmor -o /usr/share/keyrings/cloud.google.gpg
#       #       echo "deb [signed-by=/usr/share/keyrings/cloud.google.gpg] https://packages.cloud.google.com/apt cloud-sdk main" | \
#       #       sudo tee /etc/apt/sources.list.d/google-cloud-sdk.list
#       #       sudo apt-get update
#       #       sudo apt-get install -y google-cloud-sdk
#       #     else
#       #     # For Amazon Linux 2023 or other yum/dnf distros, use Google's install script
#       #       sudo -u ec2-user bash
#       #       cd ~
#       #       curl -O https://dl.google.com/dl/cloudsdk/channels/rapid/downloads/google-cloud-cli-linux-x86_64.tar.gz
#       #       tar -xf google-cloud-cli-linux-x86_64.tar.gz
#       #        ./google-cloud-sdk/install.sh --quiet
#       #       echo 'source $HOME/google-cloud-sdk/path.bash.inc' >> ~/.bashrc
#       #       echo 'source $HOME/google-cloud-sdk/completion.bash.inc' >> ~/.bashrc
#       #       source ~/.bashrc
#       #      fi

#       # - name: Install GKE auth plugin and authenticate to GCP
#       #   run: |
#       #     # Install gke plugin
#       #      gcloud components install gke-gcloud-auth-plugin --quiet

#           #  # Authenticate to GCP
#           #   echo "${{ secrets.GCP_SA_KEY }}" | base64 --decode > gcp-key.json
#           #   gcloud auth activate-service-account --key-file=gcp-key.json --quiet
#           #   # Verify
#           #   gcloud version
#           #   kubectl version --client

#         #  Clean and reconfigure Docker auth for Artifact Registry
#       - name: Clean old Docker logins (optional safeguard)
#         run: |
#           docker logout https://${{ env.GCP_REGION }}-docker.pkg.dev || true

#       - name: Ensure Artifact Registry repo exists
#         run: |
#           if ! gcloud artifacts repositories describe my-repo --location=${{ env.GCP_REGION }} >/dev/null 2>&1; then
#           gcloud artifacts repositories create my-repo \
#           --repository-format=docker \
#           --location=${{ env.GCP_REGION }} \
#           --description="Docker repo for my project"
#           echo "Waiting for repo to be ready..."
#           sleep 15
#           fi

#       - name: Docker login to GCP Artifact Registry
#         run: |
#           docker logout https://${{ env.GCP_REGION }}-docker.pkg.dev || true
#           gcloud auth configure-docker ${{ env.GCP_REGION }}-docker.pkg.dev --quiet

#         # âœ… (Optional fallback: Manual login if gcloud helper fails)
#       - name: Fallback Docker login (only if needed)
#         if: failure()
#         run: |
#           gcloud auth print-access-token | \
#           docker login -u oauth2accesstoken --password-stdin https://${{ env.GCP_REGION }}-docker.pkg.dev

#       # Build Docker Image
#       - name: Build GCP Image
#         run: |
#           docker build -t $IMAGE_NAME -f flask_app/Dockerfile .
#           docker tag $IMAGE_NAME:latest \
#             ${{ env.GCP_REGION }}-docker.pkg.dev/$GCP_PROJECT_ID/my-repo/$IMAGE_NAME:${GITHUB_SHA}

#       - name: Login to Artifact Registry
#         run: |
#           gcloud auth configure-docker ${{ env.GCP_REGION }}-docker.pkg.dev --quiet

#       - name: Push to GCP Artifact Registry
#         run: |
#           docker push \
#             ${{ env.GCP_REGION }}-docker.pkg.dev/$GCP_PROJECT_ID/my-repo/$IMAGE_NAME:${GITHUB_SHA}

#       - name: Set IMAGE_URI for GCP
#         run: |
#           echo "IMAGE_URI=${GCP_REGION}-docker.pkg.dev/${GCP_PROJECT_ID}/my-repo/${IMAGE_NAME}:${GITHUB_SHA}" >> $GITHUB_ENV

#       - name: Update Cluster
#         run: |
#           gcloud container clusters update "$GCP_CLUSTER_NAME" \
#           --region us-east4 \
#           --enable-master-authorized-networks \
#           --master-authorized-networks 0.0.0.0/0 \
#           --project prod-251618-359501
#            echo "Current kubectl context:"
#           kubectl config current-context

#       - name: Create GCP Artifact Registry Pull Secret
#         run: |
#           # Full registry URL
#            REGISTRY_URL="${GCP_REGION}-docker.pkg.dev/${GCP_PROJECT}/${GCP_REPO}"
#           kubectl create secret docker-registry gcp-creds \
#           --docker-server="${REGISTRY_URL}" \
#           --namespace default \
#           --docker-username=_json_key \
#           --docker-password="$(echo "${{ secrets.GCP_SA_KEY }}" | base64 --decode)" \
#           --dry-run=client -o yaml | kubectl apply -f -

#       - name: Update kubeconfig for GKE
#         run: |
#           echo "Updating kubeconfig for GKE cluster..."
#            gcloud container clusters get-credentials "$GCP_CLUSTER_NAME" \
#           --region "$GCP_REGION" \
#           --project "$GCP_PROJECT_ID" \

#       - name: Test GKE access
#         run: |
#           kubectl get nodes

#       # - name: Connect to ArgoCD via port-forward
#       #   run: |
#       #    kubectl -n argocd port-forward svc/argocd-server 8080:443 &
#       #    sleep 10

#       # Install ArgoCD on GCP
#       # - name: Install ArgoCD on GKE
#       #   run: |
#       #     kubectl create namespace argocd || true
#       #     kubectl apply -n argocd \
#       #       -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml
#       #     kubectl apply \
#       #       -f https://raw.githubusercontent.com/argoproj/argo-rollouts/stable/manifests/install.yaml
#       #     kubectl wait --for=condition=ready pod -n argocd --selector=app.kubernetes.io/name=argocd-server --timeout=300s

#       #     # Wait additionally until the argocd-cm exists
#       #     until kubectl get configmap argocd-cm -n argocd >/dev/null 2>&1; do
#       #        echo "Waiting for argocd-cm to be created..."
#       #        sleep 5
#       #     done

#       - name: Fetch DB credentials from Terraform outputs
#         run: |
#           # chackout master to fetch latest terraform outputs
#           git fetch origin master
#           git checkout master
#           # GCP DB credentials
#           export GCP_DB_USERNAME=$(terraform output -raw gcp_db_username)
#           export GCP_DB_PASSWORD=$(terraform output -raw gcp_db_password)
#           export GCP_DB_HOST=$(terraform output -raw gcp_db_host)
#           export GCP_DB_NAME=$(terraform output -raw gcp_db_name)

#           echo "GCP_DB_USERNAME=$GCP_DB_USERNAME" >> $GITHUB_ENV
#           echo "GCP_DB_PASSWORD=$GCP_DB_PASSWORD" >> $GITHUB_ENV
#           echo "GCP_DB_HOST=$GCP_DB_HOST" >> $GITHUB_ENV
#           echo "GCP_DB_NAME=$GCP_DB_NAME" >> $GITHUB_ENV
#           echo "BRANCH_NAME=$BRANCH_NAME" >> $GITHUB_ENV

#       - name: Apply Kubernetes Secret
#         run: |
#           git fetch origin gitop
#           git checkout gitop  
#           cd argocd/k8s
#           envsubst < secret-gcp.yaml | kubectl apply -f -
#           envsubst < argocd-rbac.yaml | kubectl apply -f -

#       - name: Apply ArgoCD ApplicationSet (AWS)
#         run: |
#           git fetch origin gitop
#           git checkout gitop  
#           cd argocd/gcp
#           kubectl apply -f flask-app-rollout.yaml -n argocd --namespace=default

#       - name: deploy ingress-gcp and service-gcp
#         run: |
#           git fetch origin gitop
#           git checkout gitop
#           cd argocd/k8s
#           kubectl apply -f ingress-gcp.yaml 
#           kubectl apply -f service-gcp.yaml

#       - name: Update kubeconfig GCP
#         run: |
#           echo "Cluster: $GCP_CLUSTER_NAME, Region: $GCP_REGION, Project: $GCP_PROJECT_ID"
#            gcloud container clusters get-credentials "$GCP_CLUSTER_NAME" \
#              --region "$GCP_REGION" \
#              --project "$GCP_PROJECT_ID"

#       # - name: Apply Kubernetes Rollout (GCP)
#       #   run: |
#       #     git fetch origin gitop
#       #     git checkout gitop
#       #     cd argocd
#       #     envsubst < flask-app-gcp-rollout.yaml | kubectl apply -f -

#       # - name: Fetch ArgoCD admin password
#       #   run: |
#       #    ARGO_PWD=$(kubectl -n argocd get secret argocd-initial-admin-secret \
#       #    -o jsonpath="{.data.password}" | base64 -d)
#       #     echo "ARGOCD_ADMIN_PASSWORD=$ARGO_PWD" >> $GITHUB_ENV

#       # - name: ArgoCD login
#       #   run: |
#       #     echo "Using ArgoCD server: localhost:8080"
#       #     argocd login localhost:8080 \
#       #     --username admin \
#       #     --password $ARGOCD_ADMIN_PASSWORD \
#       #     --insecure

#       # - name: Wait for ArgoCD core config
#       #   run: |
#       #    echo "Waiting for argocd-cm..."
#       #    until kubectl get configmap argocd-cm -n argocd >/dev/null 2>&1; do
#       #    sleep 5
#       #    done

#       #     echo "Waiting for argocd-secret..."
#       #    until kubectl get secret argocd-secret -n argocd >/dev/null 2>&1; do
#       #    sleep 5
#       #    done

#       #    echo "ArgoCD core components ready"

#       # - name: Create or Upsert ArgoCD App (GCP)
#       #   run: |
#       #     argocd app create flask-app-gcp \
#       #      --repo  https://github.com/${{ github.repository }} \
#       #      --path argocd \
#       #      --revision gitop \
#       #      --dest-server https://kubernetes.default.svc \
#       #      --dest-namespace default \
#       #       --auto-prune \
#       #       --sync-policy automated \
#       #      --upsert \
#       #      --self-heal \
#       #      --directory-recurse

#       # - name: Update ArgoCD App Image & Sync (GCP)
#       #   run: |
#       #     echo "Updating ArgoCD deployment image to $IMAGE_URI ..."
#       #     argocd app set flask-app-gcp \
#       #       --image "$IMAGE_URI" \
# #
#       #     argocd app sync flask-app-gcp

#       # ----------------------------------------
#       # ðŸš¨ SLACK NOTIFICATION FOR GCP JOB
#       # ----------------------------------------
#       - name: Slack Notify GCP Job
#         if: always()
#         env:
#           SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}
#         run: |
#           STATUS="${{ job.status }}"
#           COMMIT=$GITHUB_SHA
#           REPO=$GITHUB_REPOSITORY
#           JOB="GCP Build & Deploy"

#           curl -X POST -H 'Content-type: application/json' \
#             --data "{
#               \"text\": \"*$JOB* finished with status: *$STATUS*\nRepo: $REPO\nCommit: $COMMIT\"
#             }" \
#             $SLACK_WEBHOOK_URL

# #############################################################################################

# # Below steps to use if you do not want to use ArgoCD for deployment
# #############################################################################################

# ##### if step 8 is used, skeep step 9 and vice versa #####
# # 8ï¸âƒ£ Deploy to GKE
# # - name: Deploy to GKE
# #   run: |
# #     git fetch origin gitop
# #     git checkout gitop
# #     kubectl apply -f k8s/flask-app/deployment-gcp.yaml
# # kubectl apply -f k8s/service.yaml
# # kubectl apply -f k8s/ingress-gcp.yaml
# # kubectl apply -f k8s/secret-gcp.yaml

# # # Deploy to AWS Eks
# # - name: Deploy to AWS EKS
# #   env:
# #     AWS_REGION: ${{ env.AWS_REGION }}
# #     CLUSTER_NAME:  multi-cloud-cluster # replace with your EKS cluster name
# #   run: |
# #     # Update kubeconfig for EKS
# #      git fetch origin gitop
# #      git checkout gitop
# #      aws eks update-kubeconfig --name $CLUSTER_NAME --region $AWS_REGION
# #      kubectl apply -f k8s/flask-app/deployment-aws.yaml
# # kubectl apply -f k8s/service.yaml
# # kubectl apply -f k8s/ingress-aws.yaml
# # kubectl apply -f k8s/secret-aws.yaml

# ##### end of alternative steps #####
